---
title: "CSN - Third Lab"
author: "Kymry Burwell, Laura Cebollero"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
date: "November, 2018"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r setup, include=FALSE, echo=FALSE}
wd = getwd()
if(grepl("nora", wd)) {
    setwd("~/git/csn-labs/03")
} else {
    setwd("~/Google Drive/UPC/Fall 2018/CSN/Labs/git_labs/Complex-and-Social-Networks/03")
}
rm(wd)

source("baseMetrics.R")
source("nullHypothesisResults.R")
```

# Introduction

For this lab we are asked to verify the possible significance of network metrics applied
to global syntactic dependency trees, generated from large samples of 10 different languages.

We have chosen to work with Closeness Centrality $\mathcal{C}$ and check if this metric is significantly large when comparing the real network to randomly generated networks using two different methods: Erdős-Rényi and Switching.

Our base model is each syntactic tree created from a syntatic network example of a language $l$, which is a language contained to a total set of $n$ languages: $L = \{l_{0}, l_{1}, .., l_{n}\}$

For each language, we are going to compare its original model to two others, which will be our null models $N_{0}$:

- A binomial (Erdos-Renyi) graph generated randomly. The number of vertices and edges will be the same as those of the real network.
- A randomized graph generated using the switching method. The degree distribution of the original network is maintained in this model. 

When comparing each model to the original network, we are going to work with a traditional confidence interval of 0.95. This means we will reject the null hypothesis if the null model metric is smaller than the real network metric more than 95% of the time (p-value $\le$ 0.05). On the other hand, we will fail to reject the null hypothesis if p-value > 0.05. This p-value is going to be computed using the Monte-Carlo method explained during theory class. 

In order to keep our code tidy, the implementation can be found in different files, each one containing one function of the implementation encapsulated in it.

- ``baseMetrics.R`` contains the implementation of the base metrics presented in table 1 as well as our functions for calculating the closeness centrality metric. 
- ``binomial.R`` contains the implementation of the Erdos-Renyi model.
- ``switching.R`` contains the implementation of the switching model.
- ``switching.cpp`` contains the implementation of the switching model in `C++` (needed for the large Czech and Chinese languages)
- ``modelsComparisons.R`` contains the implementation of the comparison of the two random models to the original structure.

As you can see, we opted to use `R` for most of this lab. This is because of the many libraries availabe for quickly reading nodes, edges and creating and manipulating graphs. R also allows us to easily simplify the graph by removing the multi-edges and loops almost instantly, which biased us on at least using the iGraph package. We did however end up using `C++` for the switching model for the large languages. We will discuss the reason for this change later in the report.  


## Results
Before computing any results, since we want to work with syntactic dependency trees, we need to remove loops and
multi-edges (graph's properties not present in well-defined trees structures). Once removed, we can proceed with computing a summary table displayed below:

```{r echo=FALSE, cache=TRUE, warning=FALSE}
kable(computeBasicMetricsTable(),
            caption="\\label{tab:table1}Metrics of Global Syntatic Dependency Networks",
      col.names = c("language", "N", "E", "$\\langle k \\rangle$", "$\\delta$"),
      align=rep('c', 5))
```

In table 1 above, N is the number of vertices, E is the number of edges, $\langle k \rangle$ is the mean degree, and $\delta$ is the network density of edges. 

Next, we focus solely on the Basque language to test our implmentation and determine if we need to use any estimation procedures to speed up the computation time. First, we compute the exact closness centrality of the original data as well as the p-values for the binomial and switching methods (with T=20 iterations when computing the models). The results can be found in Table 3 below. We have combined these results with the results of all languages. 

While computing these closness centrality for the Basque langauge, it became apparant that we couldn't use our initial implementation for all languages, as the computation time was prohibitive (especially on our old computers). Computing the exact closeness for the Basque language took nearly 3 minutes for a single run. We decided that doing this for every language and 20 times for every model is infeasible. Thus, we decide to use an estimation of the closeness for the original network data. This was done using the following formula:

$$
C \approx \frac{1}{M} \sum_{i=1}^{M}C_i
$$
We know that by using a uniformly random permutation of the original vertices in the above formula, we can obtain a good estimate (with small error) when M $\approx$ 100M/N. This is good for estimating the closess centrality of our real language data. In the end, we choose $M \approx N/50$ in order to be a bit more accurate and still allow us enough time to compute the metric and test its significance for all languages. 

Now that we have an M for estimating the closeness centrality of the original networks, we want to see if we can obtain even quicker results when computing the closeness for the null hypothesis (NH) graphs by using bounding formulas. After all, in order to get accurate results, we need to test the metric agaist at least T=20 null models. The two formulas we used, bounding $C$ below and above, are as follows:
$$
C_{min} = \frac{1}{N} \sum_{i=1}^{M}C_i
$$
$$
C_{max} = \frac{1}{N} \sum_{i=1}^{M}C_i + 1 - \frac{M}{N}
$$

When computing the global closeness centrality of the NH graphs, we iterate over each vertex and compute its local closeness. During each iteration, we calculate a new $C_{min}$ and $C_{max}$. From these formulas, we know that if $C_{min} \geq C$ then it can be concluded that $C_{NH} < C$. We also know that if $C_{max} < C$ then it can be concluded that $C_{NH} < C$. Once this occurs, we can simply stop calculating the global closeness and be confident that our result is accurate. 

Taking this one step further, we want to see if the ordering of the vertices has an effect on the speed of this method. The orderings we will test are the following:

- Original vertex ordering
- Random order of vertices (Using a uniformly random permutation of the vertices)
- Increasing order by degree
- Decreasing order by degree

We test the orderings with the Basque language on the switching method NH graphs. Table 2 displays the time (in minutes) needed before the $C_{min}$ or $C_{max}$ bounds have an effect. 

```{r echo=FALSE, cache=TRUE}
kable(M_results, 
      caption="\\label{tab:table2} Computation time (in minutes) for various vertex orderings",
      col.names = c("language", "default", "randomized", "increasing degree", "decreaseing degree"),
      align=rep('c', 5))
```

As we can see, it doesn't seem that the odering has much of an effect - at least on the Basque language. Therefore, we decide to use the uniformly random ordering when computing the closeness centrality of the NH graphs for all the languages. Furthermore, we set $M_{max} = N/1000$ to speed up our computation times. $M_{max}$ is the maximum number of vertices that we will check when computing the global closness centrality of the NH graphs. 

After all of this testing, we can finally compute the closesness centrality of all lanugaes and test the signifance against null models. The final results are displayed in table 3 below. 

```{r echo=FALSE, cache=TRUE}
kable(closeness_metrics, 
      caption="\\label{tab:table3} Closeness Metrics and P-values",
      col.names = c("Language", "Closeness", "p-value (binomial)", "p-value (switching"),
      align=rep('c', 5))
```


## Discussion

Table 1 displays the basic network metrics of the different languages. As we can see, Chinese, Catalan, and Czech all have a very large number of vertices and edges, and in turn caused us the most problems during the execution phase. 

Looking at table 2, we can see that the results of the binomial (Erdos Renyi) and switching methods differ significantly. For all languages, the binomial model has a p-value of 0, signifying that the under this null hypothesis, the mean closeness centrality is significantly large. It appears the graphs created under the binomial model are much more sparse than the  original graphs. This is expected and tells us that syntatic language networks have a more more closely connected structure than binomial graphs. One reason for this is the fact that the binomial random graphs are built by adding random edges and do not adhere to the degree distribution of the original graphs. In fact, the probability of creating an edge between two edges is always the same independently of factors such as the degree of the vertices. However, in syntactic networks this probability is not the same for all the words (vertices).
In other words, the binomial model treats each vertex as equal while syntatic language networks do not. 

In stark contrast to the binomial model, the switching model produces 0 < p-value < 1 for all languages. 

And in fact, if we look at the table, we can see that all languages (except for catalan) have a $p-value \ge 0.05$. This means that we can reject the null hypothesis for those languages and thus we cannot accept that our metric (closeness centrality) is not larger on the theoreticall randomized, model.

TODO - how do languages differ in regard to C and significance test. 


## Methods (Implementation Summary)

**Using the iGraph graph structure on its own proved to be very slow  when computing a new model using the Switching method.** Because of this, we chose to work with an adjacency matrix that represents the edges between two nodes. This adjacency matrix takes into account the edges.

On our first attempt, **we tried using an adjacency edge list for each node**, since we know that it is very efficient time-wise when accessing the list of edges related to a node. However, **the computation of such structure for each new model took 5 or more minutes**, without taking into account the switching model computation. This was an **overhead that was not feasible for us, for we had to compute log(E) times E models for each language.** Thus, we opted on using an adjacency matrix, which could be computed within a couple of seconds and accessing it randomly was also immediate, although it was more hungry memory-wise.

Opting to use this method for the switching method resulted in not being able to compute the metrics for those languages with a very big numbre of vertices. Namely, Chinese was impossible for us, for it had over 40k nodes, which would need a matrix of $40.000 * 40.000$.

Although Catalan has a very close number of nodes, our computer resources allow having a matrix of $36.000 * 36.000$.

So **only two languages have been left out** on our switching computations due to lack of memory on our machines:

- **Chinese**, with over 40.000 nodes.
- **Czech**, with over 69.000 nodes.

Computing the binomial p-value  was **very** slow on those two languages as well due to their size.



## Methods (Switching Method Implementation)

In order to test the data against a NH, we utilized the switching method. This method generates a new random graph with the same degree sequence of the original graph by switching random edges. It ensures no loops or hyperedges are introduced during the switching process. A switch takes two edges, u ~ v and s ~ t and, if safe to do so, creates two new edges u ~ t and s ~ v.  

As mentioned previously, we utilized two separate data structures to implement this process: an edge list and an adjacency matrix. We used the edgelist to create the edge pairings that we would attempt to swtich. The adjacnecy matrix was used to check that the switch was safe to make. 

To start the process, we first generated two vectors of size QE (E = number of edges, Q = log(E)) with numbers chosen uniformuly at random from 1:E. These acted as the pairs of edges that we would attempt to switch. 

Next, we iterated through each of the edge pairings and checked that they were safe to switch. That is, we checked that self-loops wouldn't be created:
-  t != u
-  v != s

We also checked that no multiedges would be created:
- Edges s ~ v or u ~ t do not exist. 

If it was safe to do so, we then made the switch. Any failures that occured during this process counted towards the QE switches performed. 

During this process, we ran into prohibitively slow computation times for the Chinese and Czech languages - so slow that we couldn't continue with this implmentation. This was due to the extreme size of the adjacency matrix R created. Therefore, we decided to write the same switching procedure in `C++` and use this for Czech and Chinese. Even with `C++`, performing all 20 iterations of the switching models took upwards of 15min each. 






