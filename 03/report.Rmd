---
title: "CSN - Third Lab"
author: "Kymry Burwell, Laura Cebollero"
geometry: "left=3cm,right=3cm,top=2cm,bottom=2cm"
date: "November, 2018"
output: pdf_document
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'h')
```

```{r setup, include=FALSE, echo=FALSE}
wd = getwd()
if(grepl("nora", wd)) {
    setwd("~/git/csn-labs/03")
} else {
    setwd("~/Google Drive/UPC/Fall 2018/CSN/Labs/git_labs/Complex-and-Social-Networks/03")
}
rm(wd)

source("baseMetrics.R")
source("nullHypothesisResults.R")
```

# Introduction

For this lab we are asked to verify the possible significance of network metrics applied
to global syntactic dependency trees, generated from large samples of 10 different languages.

We have chosen to work with Closeness Centrality $\mathcal{C}$ and check if this metric is significantly large when comparing the real network to randomly generated networks using two different methods: Erdős-Rényi and Switching.

Our base model is each syntactic tree created from a syntatic network example of a language $l$, which is a language contained to a total set of $n$ languages: $L = \{l_{0}, l_{1}, .., l_{n}\}$, where $n=10$.

For each language, we are going to compare its original model to two others, which will be our null models $N_{0}$:

- A binomial (Erdos-Renyi) graph generated randomly. The number of vertices and edges will be the same as those of the real network.
- A randomized graph generated using the switching method. The degree distribution of the original network is maintained in this model. 

When comparing each model to the original network, we are going to work with a traditional confidence interval of 0.95. This means we will reject the null hypothesis if the null model metric is smaller than the real network metric more than 95% of the time (p-value $\le$ 0.05). On the other hand, we will fail to reject the null hypothesis if p-value > 0.05. This p-value is going to be computed using the Monte-Carlo method explained during theory class. 

In order to keep our code tidy, the implementation can be found in different files, each one containing one function of the implementation encapsulated in it.

- ``baseMetrics.R`` contains the implementation of the base metrics presented in table 1 as well as our functions for calculating the closeness centrality metric. 
- ``binomial.R`` contains the implementation of the Erdos-Renyi model.
- ``switching.R`` contains the implementation of the switching model.
- ``switching.cpp`` contains the implementation of the switching model in `C++` (needed for the large Czech and Chinese languages)
- ``modelsComparisons.R`` contains the implementation of the comparison of the two random models to the original structure.

As you can see, we opted to use `R` for most of this lab. This is because of the many libraries availabe for quickly reading nodes, edges and creating and manipulating graphs. R also allows us to easily simplify the graph by removing the multi-edges and loops almost instantly, which biased us on at least using the iGraph package. We did however end up using `C++` for the switching model for the large languages. We will discuss the reason for this change later in the report.  


## Results
Before computing any results, since we want to work with syntactic dependency trees, we need to remove loops and
multi-edges (graph's properties not present in well-defined trees structures). Once removed, we can proceed with computing a summary table displayed below:

```{r echo=FALSE, cache=TRUE, warning=FALSE}
kable(computeBasicMetricsTable(),
            caption="\\label{tab:table1}Metrics of Global Syntatic Dependency Networks",
      col.names = c("language", "N", "E", "$\\langle k \\rangle$", "$\\delta$"),
      align=rep('c', 5))
```

In table 1 above, N is the number of vertices, E is the number of edges, $\langle k \rangle$ is the mean degree, and $\delta$ is the network density of edges. 

Next, we focus solely on the Basque language to test our implmentation and determine if we need to use any estimation procedures to speed up the computation time. First, we compute the exact closness centrality of the original data as well as the p-values for the binomial and switching methods (with T=20 iterations when computing the models). The results can be found in Table 3 below. We have combined these results with the results of all languages. 

While computing these closness centrality for the Basque langauge, it became apparant that we couldn't use our initial implementation for all languages, as the computation time was prohibitive (especially on our old computers). Computing the exact closeness for the Basque language took nearly 3 minutes for a single run. We decided that doing this for every language and 20 times for every model is infeasible. Thus, we decide to use an estimation of the closeness for the original network data. This was done using the following formula:

$$
C \approx \frac{1}{M} \sum_{i=1}^{M}C_i
$$
We know that by using a uniformly random permutation of the original vertices in the above formula, we can obtain a good estimate (with small error) when M $\approx$ 100M/N. This is good for estimating the closess centrality of our real language data. In the end, we choose $M \approx N/50$ in order to be a bit more accurate and still allow us enough time to compute the metric and test its significance for all languages. 

Now that we have an M for estimating the closeness centrality of the original networks, we want to see if we can obtain even quicker results when computing the closeness for the null hypothesis (NH) graphs by using bounding formulas. After all, in order to get accurate results, we need to test the metric agaist at least T=20 null models. The two formulas we used, bounding $C$ below and above, are as follows:
$$
C_{min} = \frac{1}{N} \sum_{i=1}^{M}C_i
$$
$$
C_{max} = \frac{1}{N} \sum_{i=1}^{M}C_i + 1 - \frac{M}{N}
$$

When computing the global closeness centrality of the NH graphs, we iterate over each vertex and compute its local closeness. During each iteration, we calculate a new $C_{min}$ and $C_{max}$. From these formulas, we know that if $C_{min} \geq C$ then it can be concluded that $C_{NH} < C$. We also know that if $C_{max} < C$ then it can be concluded that $C_{NH} < C$. Once this occurs, we can simply stop calculating the global closeness and be confident that our result is accurate. 

Taking this one step further, we want to see if the ordering of the vertices has an effect on the speed of this method. The orderings we will test are the following:

- Original vertex ordering
- Random order of vertices (Using a uniformly random permutation of the vertices)
- Increasing order by degree
- Decreasing order by degree

We test the orderings with the Basque language, since it is the smallest one, on the switching method NH graphs. Table 2 displays the time (in minutes) needed before the $C_{min}$ or $C_{max}$ bounds have an effect. 

```{r echo=FALSE, cache=TRUE}
kable(M_results, 
      caption="\\label{tab:table2} Computation time (in minutes) for various vertex orderings",
      col.names = c("Language", "Default", "Randomized", "Increasing degree", "Decreasing degree"),
      align=rep('c', 5))
```

As we can see, it doesn't seem that the ordering has much of an effect - at least on the Basque language. Therefore, we decide to use the uniformly random ordering when computing the closeness centrality of the NH graphs for all the languages. Furthermore, we set $M_{max} = N/1000$ to speed up our computation times. $M_{max}$ is the maximum number of vertices that we will check when computing the global closeness centrality of the NH graphs. 

After all of this testing, we can finally compute the closeness centrality of all languages and test the significance against null models. The final results are displayed in table 3 below. 

```{r echo=FALSE, cache=TRUE}
kable(closeness_metrics, 
      caption="\\label{tab:table3} Closeness Metrics and P-values",
      col.names = c("Language", "Closeness", "p-value (Binomial)", "p-value (Switching)"),
      align=rep('c', 5))
```


## Discussion

Table 1 displays the basic network metrics of the different languages. As we can see, Chinese, Catalan, and Czech all have a very large number of vertices and edges, and in turn caused us the most problems during the execution phase. 

Looking at table 2, we can see that the results of the binomial (Erdős-Rényi) and switching methods differ significantly. For all languages, the binomial model has a p-value of 0, signifying that the under this null hypothesis, the mean closeness centrality is significantly large. 

Our Binomial models have been created with the same number of vertices and edges and always contemplates the probability of creating an edge between two vertices as the same. That is, it does not take into account other factors such as the existing degree of a vertex in the moment of adding a new edge. In other words, the binomial model treats each vertex as equal while syntatic language networks do not.  Due to this,  the graphs created under the binomial model are much more sparse than the  original graphs, and thus, the closeness metric is always smaller.


In stark contrast to the binomial model, the switching model produces a varying p-value such that $$0 < p-value < 1$$ for all languages. 

And in fact interestingly enough, if we look at the table, we can see that all languages, except for catalan, have a $p-value \ge 0.05$. This means that many of the randomized models created with the switching method have a larger closeness metric, thus resulting in a p-value above 0.05.

Since the switching model starts from the original graph, it makes sense that the closeness centrality may be larger in many cases. First of all, because it preserves the degree sequence. This is a great advantage of ensuring a closeness at least very close to the original one. And second, since when we are performing a switch we check that it is correct before actually saving it, we are also enforcing a graph somewhat similar to the previous one on each iteration.

We have tried to see why the p-value is so low on the case on Catalan and not on the other languages, and we have not been able to arrive to a solid conclusion other than attributing this outlier to the randomness of the switching method.

As for the closeness centrality itself on each language network, we can see that the best connected languages in regards to closeness centrality metric are Catalan, Chinese, English, Italian and Turkish. Opposite to Basque and Czech, which have a lower values values of 0.27 and 0.28 respectively.

It is interesting to point how this metric work as it does, for Basque and Czech are actually on extreme opposite ends in regards of the number of vertices they have in their network, with Basque having the lowest value with a total of 12k meanwhile Czech is the largest one by difference with 69k. This means that their nodes are connected in such way that averaging their inverse geodesic distances gives a similar numeric result.


## Methods (Overall implementation Summary)

Using the iGraph graph structure on its own proved to be very slow  when computing a new model using the Switching method. Because of this, we chose to work with an adjacency matrix that represents the edges between two nodes, which is a more memory-hungry method but also faster to compute.

Even though on our first attempt we tried using an adjacency edge list for each node, since we know that it is very efficient time-wise when accessing the list of edges related to a node, we could not settle on it. The computation of such structure converting it from the iGraph structure took 5 or more minutes  for each new model, without taking into account the switching model computation. This was an overhead that was not feasible for us, for we had to compute 20 times the log(E) times E models for each language. Thus, we opted on using an adjacency matrix, which could be computed within a couple of seconds and accessing it randomly was also immediate, although it was more hungry memory-wise as previously stated.

Opting to use this method for the switching method resulted in not being able to compute the metrics for those languages with a very big number of vertices in R. Namely, Chinese was impossible, for it had over 40k nodes, which would need a matrix of $40.000 * 40.000$.

Although Catalan has a very close number of nodes, our computer resources allow having a matrix of $36.000 * 36.000$.

So only two languages have been left out on our switching computations on R due to lack of memory on our machines:

- Chinese, with over 40.000 nodes.
- Czech, with over 69.000 nodes.

Computing the binomial p-value  was very slow on those two languages as well due to their size.

Not wanting to leave these languages totally out we opted on implementing the switching model on `C++`, which manages the memory resources better and is also more efficient.


## Methods (Switching Method Implementation)

The switching method generates a new random graph with the same degree sequence of the original graph by switching random edges, as previously explained.

It ensures that no loops or hyperedges are introduced during the switching process. A switch takes two edges, u ~ v and s ~ t and, if safe to do so, creates two new edges u ~ t and s ~ v.  

As mentioned previously, we utilized two separate data structures to implement this process: an edge list and an adjacency matrix. We used the edgelist to create the edge pairings that we would attempt to switch. The adjacency matrix was used to check that the switch was safe to make. 

To start the process, we first generated two vectors of size QE (E = number of edges, Q = log(E)) with numbers chosen uniformuly at random from 1:E. These acted as the pairs of edges that we would attempt to switch. 

Next, we iterated through each of the edge pairings and checked that they were safe to switch. That is, we checked that self-loops wouldn't be created:

-  t != u
-  v != s

We also checked that no multiedges would be created:

- Edges s ~ v or u ~ t do not exist. 

If it was safe to do so, we then made the switch. Any failures that occured during this process counted towards the QE switches performed. 

During this process, we ran into prohibitively slow computation times for the Chinese and Czech languages - so slow that we couldn't continue with this implmentation. This was due to the extreme size of the adjacency matrix R created. Therefore, we decided to write the same switching procedure in `C++` and use this for Czech and Chinese. Even with `C++`, performing all 20 iterations of the switching models took upwards of 15min each. 


## Final reflections and conclusions

After thinking about the computation times, we realised that we are actually somewhat lucky that the small, original networks we are working on are sparse as seen in table 1. Were they more dense, the switching computation would be slower, even for the small networks such as Arabic or Basque, for they would have way more edges to try a switching on.

The switching method has proved to produce a randomized method closer to the original one, although its computation times is way larger than just computing a Binomimal one. However, this method should be taken into account when you want some specific, randomized network based on another one (i.e. creating a new friendship network based on a real, observed one for research purposes). The Binomial method is fast to compute but not reliable when wanting such specific networks, not to mention that it may create loops and multiedges.






